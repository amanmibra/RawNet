{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a0b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd3c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = pathlib.Path.home()\n",
    "WAVEFAKE=os.path.abspath(os.path.join(home, \"Desktop\", \"deep-truth\", \"wavefake\"))\n",
    "\n",
    "\n",
    "REAL_SAMPLES=os.path.join(home, \"ml-sandbox\", \"VoID\", \"data\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae20221",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 3.0423456e-05,\n",
       "        3.1125106e-05, 0.0000000e+00], dtype=float32),\n",
       " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 7.6964752e-05,\n",
       "        1.0224331e-04, 0.0000000e+00], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 4.0697127e-05,\n",
       "        3.3644861e-05, 3.1134885e-05], dtype=float32),\n",
       " array([ 0.        ,  0.        ,  0.        , ..., -0.00025719,\n",
       "        -0.00047243,  0.        ], dtype=float32),\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -8.4769024e-05, -1.0100049e-04,  0.0000000e+00], dtype=float32),\n",
       " array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -3.20843938e-05, -1.10914925e-05,  0.00000000e+00], dtype=float32),\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -3.0254517e-07,  7.2804220e-08,  0.0000000e+00], dtype=float32),\n",
       " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 4.4907063e-07,\n",
       "        3.3877618e-07, 0.0000000e+00], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0.        , 0.        , 0.        , ..., 0.00015859, 0.00011694,\n",
       "        0.        ], dtype=float32),\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -4.8092616e-08,  3.8005560e-08,  0.0000000e+00], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 6.3585248e-05,\n",
       "        4.2983986e-05, 0.0000000e+00], dtype=float32),\n",
       " array([0.000000e+00, 0.000000e+00, 0.000000e+00, ..., 8.717886e-06,\n",
       "        3.544239e-05, 0.000000e+00], dtype=float32),\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -2.4423134e-08,  2.1809372e-08,  0.0000000e+00], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       " array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "        -3.3849756e-06,  2.2563725e-06, -1.4532126e-06], dtype=float32),\n",
       " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # TEST - will replace with torchaudio dataset\n",
    "# # collect real audio samples\n",
    "\n",
    "# samples = []\n",
    "# for name in [\"aman\", \"imran\", \"labib\"]:\n",
    "#     directory = os.path.join(REAL_SAMPLES, name)\n",
    "#     for file_name in os.listdir(directory):\n",
    "#         audio_path = os.path.join(directory, file_name)\n",
    "#         audio, sr = librosa.load(audio_path, sr=16000)\n",
    "#         samples.append(audio)\n",
    "     \n",
    "# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5987125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.RawNet3 import RawNet3, Bottle2neck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb2058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.encoder_type ECA\n"
     ]
    }
   ],
   "source": [
    "model = RawNet3(\n",
    "        Bottle2neck,\n",
    "        model_scale=8,\n",
    "        context=True,\n",
    "        summed=True,\n",
    "        encoder_type=\"ECA\",\n",
    "        nOut=256,\n",
    "        out_bn=False,\n",
    "        sinc_stride=10,\n",
    "        log_sinc=True,\n",
    "        norm_sinc=\"mean\",\n",
    "        grad_mult=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e605a5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "        torch.load(\n",
    "            \"./../model.pt\",\n",
    "            map_location=lambda storage, loc: storage,\n",
    "        )[\"model\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e59245f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emb(sample, n_samples=16000*3, n_segments=10):\n",
    "    \n",
    "    if (len(sample) < n_samples):  # RawNet3 was trained using utterances of 3 seconds\n",
    "        shortage = n_samples - len(sample) + 1\n",
    "        sample = np.pad(sample, (0, shortage), \"wrap\")\n",
    "    \n",
    "    audios = []\n",
    "    startframe = np.linspace(0, len(audio) - n_samples, num=n_segments)\n",
    "    for asf in startframe:\n",
    "        audios.append(audio[int(asf) : int(asf) + n_samples])\n",
    "\n",
    "    audios = torch.from_numpy(np.stack(audios, axis=0).astype(np.float32))\n",
    "    with torch.no_grad():\n",
    "        output = model(audios)\n",
    "\n",
    "    return output.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e48425aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0059,  0.0090, -0.0023,  0.0081,  0.0023,  0.0036,  0.0049, -0.0045,\n",
       "         0.0051,  0.0072,  0.0104,  0.0033,  0.0137, -0.0234, -0.0188, -0.0025,\n",
       "        -0.0001,  0.0047,  0.0019, -0.0075,  0.0012, -0.0042, -0.0053,  0.0049,\n",
       "         0.0055,  0.0121,  0.0043, -0.0116, -0.0047, -0.0013,  0.0094,  0.0050,\n",
       "        -0.0027, -0.0159,  0.0099,  0.0012, -0.0027, -0.0038, -0.0107,  0.0043,\n",
       "         0.0021,  0.0142, -0.0028,  0.0063,  0.0062,  0.0098,  0.0036,  0.0124,\n",
       "        -0.0005, -0.0039,  0.0123, -0.0004, -0.0064, -0.0090, -0.0076,  0.0012,\n",
       "        -0.0114,  0.0104, -0.0081, -0.0004,  0.0025,  0.0135,  0.0069,  0.0007,\n",
       "        -0.0011,  0.0016,  0.0184,  0.0016, -0.0028, -0.0030,  0.0008, -0.0058,\n",
       "         0.0059, -0.0177,  0.0050,  0.0172, -0.0198,  0.0137, -0.0146, -0.0078,\n",
       "        -0.0040,  0.0049, -0.0060, -0.0079, -0.0098, -0.0013,  0.0005,  0.0041,\n",
       "        -0.0016, -0.0067,  0.0085, -0.0050,  0.0024,  0.0058,  0.0057,  0.0184,\n",
       "         0.0060,  0.0049, -0.0006,  0.0026, -0.0083,  0.0076, -0.0096, -0.0013,\n",
       "        -0.0077, -0.0070,  0.0003, -0.0126,  0.0006,  0.0007, -0.0084, -0.0166,\n",
       "         0.0167,  0.0053, -0.0067, -0.0168,  0.0121, -0.0073,  0.0025,  0.0006,\n",
       "         0.0005,  0.0129, -0.0035, -0.0005, -0.0055,  0.0009, -0.0042,  0.0034,\n",
       "        -0.0086,  0.0062, -0.0056, -0.0033,  0.0008, -0.0041, -0.0106,  0.0064,\n",
       "        -0.0119,  0.0038, -0.0002, -0.0002,  0.0104,  0.0137,  0.0108, -0.0075,\n",
       "        -0.0143,  0.0154, -0.0004, -0.0079, -0.0007,  0.0043, -0.0064, -0.0027,\n",
       "        -0.0063, -0.0018,  0.0072,  0.0014, -0.0011, -0.0067,  0.0033, -0.0012,\n",
       "         0.0020,  0.0113, -0.0027, -0.0030,  0.0052,  0.0050, -0.0088,  0.0106,\n",
       "        -0.0038, -0.0130, -0.0083,  0.0135, -0.0134,  0.0007,  0.0087, -0.0080,\n",
       "         0.0020,  0.0067,  0.0062, -0.0053, -0.0099, -0.0155, -0.0071, -0.0031,\n",
       "        -0.0102,  0.0107, -0.0111, -0.0085, -0.0005, -0.0079,  0.0098, -0.0028,\n",
       "         0.0050, -0.0035,  0.0131, -0.0089,  0.0162, -0.0075, -0.0027,  0.0134,\n",
       "        -0.0042,  0.0060,  0.0131,  0.0021,  0.0051,  0.0014,  0.0021, -0.0103,\n",
       "         0.0082, -0.0135, -0.0026, -0.0122,  0.0113,  0.0104, -0.0036, -0.0068,\n",
       "         0.0105, -0.0015,  0.0127, -0.0031, -0.0092, -0.0004,  0.0120, -0.0013,\n",
       "         0.0002,  0.0162,  0.0095, -0.0006, -0.0078, -0.0042,  0.0051,  0.0011,\n",
       "        -0.0010,  0.0050, -0.0041,  0.0073,  0.0101,  0.0122, -0.0026,  0.0044,\n",
       "        -0.0116, -0.0037, -0.0060,  0.0020,  0.0059,  0.0105, -0.0047, -0.0044,\n",
       "         0.0007,  0.0081,  0.0124, -0.0020,  0.0037,  0.0043,  0.0052,  0.0102])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavefake_sample_file = os.path.join(WAVEFAKE, \"ljspeech_hifiGAN\", \"LJ001-0001_generated.wav\")\n",
    "\n",
    "wavefake_audio, sr = librosa.load(wavefake_sample_file, sr=16000)\n",
    "fake_emb = extract_emb(wavefake_audio)\n",
    "fake_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b59ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "for name in [\"aman\", \"imran\", \"labib\"]:\n",
    "    directory = os.path.join(REAL_SAMPLES, name)\n",
    "    cosines = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        audio_path = os.path.join(directory, file_name)\n",
    "        audio, sr = librosa.load(audio_path, sr=16000)\n",
    "        \n",
    "        emb = extract_emb(audio)\n",
    "        cosines.append(cos(fake_emb, emb))\n",
    "    samples.append(cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1569185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05042eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
